<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Week 7</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div id="header">
<h1 class="title">Week 7</h1>
</div>
<p>Use the <a href="https://github.com/dhesse/STK-INF4000-templates">templates</a> repository for boilerplate code.</p>
<h1 id="apache-spark">Apache Spark</h1>
<ol style="list-style-type: decimal">
<li>Download Apache Spark from the [Spark website][sprk]. You want the latest Spark version (currently 2.1.0), pre-built for Hadoop 2.7 and later, choosing a direct download. This should give you a file named <code>spark-2.1.0-bin-hadoop2.7.tgz</code>. Extract this file.</li>
<li><p>Open the <code>pyspark</code> shell, issuing the command</p>
<pre><code>/path/to/spark-2.1.0-bin-hadoop2.7/bin/pyspark</code></pre></li>
</ol>
<p>Make sure you have the spark context <code>sc</code> available and can issue simple commands like <code>sc.parallelize(range(10))</code>. 3. Now try to run Spark in a Jupyter Notebook issuing in your shell (Mac/Linux, if you're on Windows the best option is to run a VM)</p>
<pre><code>    export PYSPARK_DRIVER_PYTHON=jupyter
    export PYSPARK_DRIVER_PYTHON_OPTS=notebook
    /path/to/spark-2.1.0-bin-hadoop2.7/bin/pyspark</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Write a distance metric for points represented by arbitrary-length arrays (e.g. using the 2-norm <span class="math inline">\(d(x,y) = \|x-y\|_2\)</span>.</li>
<li>Write a function that, given a RDD containing records of the form <code>[k, (X1, X2, ...)]</code>, returns a transformed RDD such that all columns <code>Xi</code> have zero mean and unit variance. Assume that each value <code>(X1, X2, ...)</code> of the RDD is a <code>numpy</code> array.</li>
<li>Implement a K-Nearest neighbor classifier in Spark. Your input is a RDD containing records of the form [l, y], an integer k and a point</li>
</ol>
<ol start="24" style="list-style-type: lower-alpha">
<li>Write a function that finds the k closest y to x in the RDD according the metric passed as argument and returns the average value for l of those.</li>
</ol>
<h1 id="python-if-covered-in-class">Python (if covered in class)</h1>
<ol style="list-style-type: decimal">
<li>Write a function returning the unique elements from a iterable (e.g. list, tuple or similar).</li>
<li>Write a generator returning the first <span class="math inline">\(N + 1\)</span> elements of the Fibonacci series given starting with values <span class="math inline">\(k_0\)</span> and <span class="math inline">\(k_1\)</span>, such that <span class="math display">\[k_i = k_{i-1} + k_{i-2},\quad i = 2, \ldots, N\]</span>.</li>
<li>Write a function that calls another function <code>f</code> passed as argument to yours. It should try to call <code>f</code> and return the result, or <code>None</code> if <code>f</code> raises a <code>ValueError</code>. Any other error should be ignored.</li>
</ol>
</body>
</html>
